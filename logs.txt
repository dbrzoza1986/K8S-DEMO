
==> Audit <==
|--------------|-----------------|----------|-------|---------|----------------------|----------------------|
|   Command    |      Args       | Profile  | User  | Version |      Start Time      |       End Time       |
|--------------|-----------------|----------|-------|---------|----------------------|----------------------|
| start        | --driver docker | minikube | dawid | v1.33.1 | 25 Aug 24 17:25 CEST | 25 Aug 24 17:26 CEST |
| update-check |                 | minikube | dawid | v1.33.1 | 25 Aug 24 17:29 CEST | 25 Aug 24 17:29 CEST |
| ip           |                 | minikube | dawid | v1.33.1 | 25 Aug 24 18:37 CEST | 25 Aug 24 18:37 CEST |
| ip           |                 | minikube | dawid | v1.33.1 | 25 Aug 24 18:40 CEST | 25 Aug 24 18:40 CEST |
| ip           |                 | minikube | dawid | v1.33.1 | 25 Aug 24 18:44 CEST | 25 Aug 24 18:44 CEST |
| start        |                 | minikube | dawid | v1.33.1 | 25 Aug 24 18:53 CEST | 25 Aug 24 18:54 CEST |
| ip           |                 | minikube | dawid | v1.33.1 | 25 Aug 24 18:54 CEST | 25 Aug 24 18:54 CEST |
| service      | webapp-service  | minikube | dawid | v1.33.1 | 25 Aug 24 20:01 CEST |                      |
| stop         |                 | minikube | dawid | v1.33.1 | 25 Aug 24 20:04 CEST | 25 Aug 24 20:04 CEST |
| service      | webapp-service  | minikube | dawid | v1.33.1 | 25 Aug 24 20:04 CEST |                      |
| start        |                 | minikube | dawid | v1.33.1 | 25 Aug 24 20:04 CEST | 25 Aug 24 20:04 CEST |
| service      | webapp-service  | minikube | dawid | v1.33.1 | 25 Aug 24 20:06 CEST |                      |
|--------------|-----------------|----------|-------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/08/25 20:04:42
Running on machine: Dawids-MacBook-Air
Binary: Built with gc go1.22.3 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0825 20:04:42.263152   62765 out.go:291] Setting OutFile to fd 1 ...
I0825 20:04:42.263360   62765 out.go:343] isatty.IsTerminal(1) = true
I0825 20:04:42.263364   62765 out.go:304] Setting ErrFile to fd 2...
I0825 20:04:42.263367   62765 out.go:343] isatty.IsTerminal(2) = true
I0825 20:04:42.263720   62765 root.go:338] Updating PATH: /Users/dawid/.minikube/bin
W0825 20:04:42.263794   62765 root.go:314] Error reading config file at /Users/dawid/.minikube/config/config.json: open /Users/dawid/.minikube/config/config.json: no such file or directory
I0825 20:04:42.265711   62765 out.go:298] Setting JSON to false
I0825 20:04:42.298760   62765 start.go:129] hostinfo: {"hostname":"Dawids-MacBook-Air.local","uptime":124086,"bootTime":1724484996,"procs":506,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.0","kernelVersion":"24.0.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"e603a9bd-e21f-55bc-b415-35f8917c016e"}
W0825 20:04:42.298915   62765 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0825 20:04:42.306668   62765 out.go:177] üòÑ  minikube v1.33.1 on Darwin 15.0 (arm64)
I0825 20:04:42.313743   62765 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0825 20:04:42.314044   62765 notify.go:220] Checking for updates...
I0825 20:04:42.314624   62765 driver.go:392] Setting default libvirt URI to qemu:///system
I0825 20:04:42.336438   62765 docker.go:122] docker version: linux-27.1.1:Docker Desktop 4.33.0 (160616)
I0825 20:04:42.336512   62765 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0825 20:04:42.569555   62765 info.go:266] docker info: {ID:bc0ffdb5-5308-4867-a137-9f0298a2016c Containers:20 ContainersRunning:19 ContainersPaused:0 ContainersStopped:1 Images:19 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:172 OomKillDisable:false NGoroutines:181 SystemTime:2024-08-25 18:04:42.557526419 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.0-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8219820032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/dawid/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/dawid/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/Users/dawid/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/Users/dawid/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:/Users/dawid/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:/Users/dawid/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/dawid/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/Users/dawid/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/dawid/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/Users/dawid/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/dawid/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I0825 20:04:42.573529   62765 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0825 20:04:42.576736   62765 start.go:297] selected driver: docker
I0825 20:04:42.576739   62765 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0825 20:04:42.576908   62765 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0825 20:04:42.576960   62765 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0825 20:04:42.638998   62765 info.go:266] docker info: {ID:bc0ffdb5-5308-4867-a137-9f0298a2016c Containers:20 ContainersRunning:19 ContainersPaused:0 ContainersStopped:1 Images:19 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:172 OomKillDisable:false NGoroutines:181 SystemTime:2024-08-25 18:04:42.629581211 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.0-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8219820032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/dawid/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/dawid/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/Users/dawid/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/Users/dawid/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:/Users/dawid/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:/Users/dawid/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/dawid/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/Users/dawid/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/dawid/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/Users/dawid/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/dawid/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I0825 20:04:42.639331   62765 cni.go:84] Creating CNI manager for ""
I0825 20:04:42.639338   62765 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0825 20:04:42.639368   62765 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0825 20:04:42.643491   62765 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0825 20:04:42.648591   62765 cache.go:121] Beginning downloading kic base image for docker with docker
I0825 20:04:42.651471   62765 out.go:177] üöú  Pulling base image v0.0.44 ...
I0825 20:04:42.657543   62765 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0825 20:04:42.657562   62765 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0825 20:04:42.657577   62765 preload.go:147] Found local preload: /Users/dawid/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0825 20:04:42.657768   62765 cache.go:56] Caching tarball of preloaded images
I0825 20:04:42.658351   62765 preload.go:173] Found /Users/dawid/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0825 20:04:42.658382   62765 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0825 20:04:42.658464   62765 profile.go:143] Saving config to /Users/dawid/.minikube/profiles/minikube/config.json ...
I0825 20:04:42.760023   62765 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I0825 20:04:42.760398   62765 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local cache directory
I0825 20:04:42.760431   62765 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local cache directory, skipping pull
I0825 20:04:42.760598   62765 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in cache, skipping pull
I0825 20:04:42.760603   62765 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e as a tarball
I0825 20:04:42.760605   62765 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from local cache
I0825 20:04:47.026649   62765 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from cached tarball
I0825 20:04:47.026730   62765 cache.go:194] Successfully downloaded all kic artifacts
I0825 20:04:47.027193   62765 start.go:360] acquireMachinesLock for minikube: {Name:mkb07f2732e459fe6fca24f3a9ec3b7df154439d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0825 20:04:47.027668   62765 start.go:364] duration metric: took 415.209¬µs to acquireMachinesLock for "minikube"
I0825 20:04:47.027728   62765 start.go:96] Skipping create...Using existing machine configuration
I0825 20:04:47.027734   62765 fix.go:54] fixHost starting: 
I0825 20:04:47.028531   62765 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 20:04:47.048971   62765 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0825 20:04:47.049007   62765 fix.go:138] unexpected machine state, will restart: <nil>
I0825 20:04:47.057044   62765 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0825 20:04:47.059046   62765 cli_runner.go:164] Run: docker start minikube
I0825 20:04:47.385173   62765 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 20:04:47.409329   62765 kic.go:430] container "minikube" state is running.
I0825 20:04:47.410141   62765 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0825 20:04:47.426175   62765 profile.go:143] Saving config to /Users/dawid/.minikube/profiles/minikube/config.json ...
I0825 20:04:47.426566   62765 machine.go:94] provisionDockerMachine start ...
I0825 20:04:47.426781   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:47.459780   62765 main.go:141] libmachine: Using SSH client type: native
I0825 20:04:47.460853   62765 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101327180] 0x1013299e0 <nil>  [] 0s} 127.0.0.1 58221 <nil> <nil>}
I0825 20:04:47.460861   62765 main.go:141] libmachine: About to run SSH command:
hostname
I0825 20:04:47.464643   62765 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0825 20:04:50.589045   62765 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0825 20:04:50.590039   62765 ubuntu.go:169] provisioning hostname "minikube"
I0825 20:04:50.590386   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:50.611416   62765 main.go:141] libmachine: Using SSH client type: native
I0825 20:04:50.611554   62765 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101327180] 0x1013299e0 <nil>  [] 0s} 127.0.0.1 58221 <nil> <nil>}
I0825 20:04:50.611558   62765 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0825 20:04:50.744837   62765 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0825 20:04:50.744980   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:50.765126   62765 main.go:141] libmachine: Using SSH client type: native
I0825 20:04:50.765270   62765 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101327180] 0x1013299e0 <nil>  [] 0s} 127.0.0.1 58221 <nil> <nil>}
I0825 20:04:50.765277   62765 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0825 20:04:50.888500   62765 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0825 20:04:50.888521   62765 ubuntu.go:175] set auth options {CertDir:/Users/dawid/.minikube CaCertPath:/Users/dawid/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/dawid/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/dawid/.minikube/machines/server.pem ServerKeyPath:/Users/dawid/.minikube/machines/server-key.pem ClientKeyPath:/Users/dawid/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/dawid/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/dawid/.minikube}
I0825 20:04:50.888552   62765 ubuntu.go:177] setting up certificates
I0825 20:04:50.888563   62765 provision.go:84] configureAuth start
I0825 20:04:50.888671   62765 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0825 20:04:50.908957   62765 provision.go:143] copyHostCerts
I0825 20:04:50.909065   62765 exec_runner.go:144] found /Users/dawid/.minikube/ca.pem, removing ...
I0825 20:04:50.909069   62765 exec_runner.go:203] rm: /Users/dawid/.minikube/ca.pem
I0825 20:04:50.909174   62765 exec_runner.go:151] cp: /Users/dawid/.minikube/certs/ca.pem --> /Users/dawid/.minikube/ca.pem (1074 bytes)
I0825 20:04:50.909598   62765 exec_runner.go:144] found /Users/dawid/.minikube/cert.pem, removing ...
I0825 20:04:50.909600   62765 exec_runner.go:203] rm: /Users/dawid/.minikube/cert.pem
I0825 20:04:50.909666   62765 exec_runner.go:151] cp: /Users/dawid/.minikube/certs/cert.pem --> /Users/dawid/.minikube/cert.pem (1115 bytes)
I0825 20:04:50.909977   62765 exec_runner.go:144] found /Users/dawid/.minikube/key.pem, removing ...
I0825 20:04:50.909980   62765 exec_runner.go:203] rm: /Users/dawid/.minikube/key.pem
I0825 20:04:50.910042   62765 exec_runner.go:151] cp: /Users/dawid/.minikube/certs/key.pem --> /Users/dawid/.minikube/key.pem (1679 bytes)
I0825 20:04:50.910273   62765 provision.go:117] generating server cert: /Users/dawid/.minikube/machines/server.pem ca-key=/Users/dawid/.minikube/certs/ca.pem private-key=/Users/dawid/.minikube/certs/ca-key.pem org=dawid.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0825 20:04:51.079720   62765 provision.go:177] copyRemoteCerts
I0825 20:04:51.079950   62765 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0825 20:04:51.079982   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.095911   62765 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58221 SSHKeyPath:/Users/dawid/.minikube/machines/minikube/id_rsa Username:docker}
I0825 20:04:51.178730   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0825 20:04:51.193253   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0825 20:04:51.204220   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0825 20:04:51.214393   62765 provision.go:87] duration metric: took 325.814125ms to configureAuth
I0825 20:04:51.214406   62765 ubuntu.go:193] setting minikube options for container-runtime
I0825 20:04:51.214559   62765 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0825 20:04:51.214599   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.228294   62765 main.go:141] libmachine: Using SSH client type: native
I0825 20:04:51.228424   62765 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101327180] 0x1013299e0 <nil>  [] 0s} 127.0.0.1 58221 <nil> <nil>}
I0825 20:04:51.228429   62765 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0825 20:04:51.345880   62765 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0825 20:04:51.345891   62765 ubuntu.go:71] root file system type: overlay
I0825 20:04:51.346049   62765 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0825 20:04:51.346159   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.365962   62765 main.go:141] libmachine: Using SSH client type: native
I0825 20:04:51.366102   62765 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101327180] 0x1013299e0 <nil>  [] 0s} 127.0.0.1 58221 <nil> <nil>}
I0825 20:04:51.366134   62765 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0825 20:04:51.489583   62765 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0825 20:04:51.489691   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.510353   62765 main.go:141] libmachine: Using SSH client type: native
I0825 20:04:51.510507   62765 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x101327180] 0x1013299e0 <nil>  [] 0s} 127.0.0.1 58221 <nil> <nil>}
I0825 20:04:51.510514   62765 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0825 20:04:51.632182   62765 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0825 20:04:51.632210   62765 machine.go:97] duration metric: took 4.205581834s to provisionDockerMachine
I0825 20:04:51.632221   62765 start.go:293] postStartSetup for "minikube" (driver="docker")
I0825 20:04:51.632232   62765 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0825 20:04:51.632373   62765 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0825 20:04:51.632439   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.654766   62765 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58221 SSHKeyPath:/Users/dawid/.minikube/machines/minikube/id_rsa Username:docker}
I0825 20:04:51.750061   62765 ssh_runner.go:195] Run: cat /etc/os-release
I0825 20:04:51.751783   62765 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0825 20:04:51.751812   62765 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0825 20:04:51.751819   62765 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0825 20:04:51.751824   62765 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0825 20:04:51.751830   62765 filesync.go:126] Scanning /Users/dawid/.minikube/addons for local assets ...
I0825 20:04:51.751986   62765 filesync.go:126] Scanning /Users/dawid/.minikube/files for local assets ...
I0825 20:04:51.752049   62765 start.go:296] duration metric: took 119.822375ms for postStartSetup
I0825 20:04:51.752133   62765 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0825 20:04:51.755336   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.774983   62765 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58221 SSHKeyPath:/Users/dawid/.minikube/machines/minikube/id_rsa Username:docker}
I0825 20:04:51.856150   62765 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0825 20:04:51.858348   62765 fix.go:56] duration metric: took 4.830548958s for fixHost
I0825 20:04:51.858358   62765 start.go:83] releasing machines lock for "minikube", held for 4.830615958s
I0825 20:04:51.858453   62765 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0825 20:04:51.879650   62765 ssh_runner.go:195] Run: cat /version.json
I0825 20:04:51.879697   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.880271   62765 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0825 20:04:51.880407   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:51.893336   62765 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58221 SSHKeyPath:/Users/dawid/.minikube/machines/minikube/id_rsa Username:docker}
I0825 20:04:51.893335   62765 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58221 SSHKeyPath:/Users/dawid/.minikube/machines/minikube/id_rsa Username:docker}
I0825 20:04:51.975543   62765 ssh_runner.go:195] Run: systemctl --version
I0825 20:04:52.117847   62765 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0825 20:04:52.120475   62765 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0825 20:04:52.130662   62765 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0825 20:04:52.130779   62765 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0825 20:04:52.136712   62765 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0825 20:04:52.136986   62765 start.go:494] detecting cgroup driver to use...
I0825 20:04:52.137006   62765 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0825 20:04:52.137757   62765 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0825 20:04:52.145780   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0825 20:04:52.150522   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0825 20:04:52.154791   62765 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0825 20:04:52.154911   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0825 20:04:52.159345   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0825 20:04:52.163753   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0825 20:04:52.168236   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0825 20:04:52.176194   62765 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0825 20:04:52.185309   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0825 20:04:52.190517   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0825 20:04:52.195616   62765 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0825 20:04:52.200447   62765 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0825 20:04:52.204322   62765 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0825 20:04:52.208964   62765 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 20:04:52.268341   62765 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0825 20:04:52.326676   62765 start.go:494] detecting cgroup driver to use...
I0825 20:04:52.326697   62765 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0825 20:04:52.326821   62765 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0825 20:04:52.335728   62765 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0825 20:04:52.335827   62765 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0825 20:04:52.343474   62765 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0825 20:04:52.350972   62765 ssh_runner.go:195] Run: which cri-dockerd
I0825 20:04:52.352987   62765 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0825 20:04:52.356713   62765 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0825 20:04:52.366347   62765 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0825 20:04:52.405417   62765 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0825 20:04:52.449899   62765 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0825 20:04:52.450037   62765 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0825 20:04:52.457442   62765 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 20:04:52.491640   62765 ssh_runner.go:195] Run: sudo systemctl restart docker
I0825 20:04:52.673364   62765 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0825 20:04:52.678342   62765 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0825 20:04:52.683631   62765 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0825 20:04:52.688354   62765 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0825 20:04:52.719712   62765 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0825 20:04:52.750233   62765 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 20:04:52.781849   62765 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0825 20:04:52.801837   62765 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0825 20:04:52.806528   62765 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 20:04:52.836055   62765 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0825 20:04:52.913161   62765 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0825 20:04:52.913522   62765 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0825 20:04:52.915644   62765 start.go:562] Will wait 60s for crictl version
I0825 20:04:52.915719   62765 ssh_runner.go:195] Run: which crictl
I0825 20:04:52.917739   62765 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0825 20:04:52.940035   62765 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0825 20:04:52.940102   62765 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0825 20:04:52.953433   62765 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0825 20:04:52.967304   62765 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0825 20:04:52.967726   62765 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0825 20:04:53.050635   62765 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0825 20:04:53.051075   62765 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0825 20:04:53.053799   62765 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0825 20:04:53.059870   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0825 20:04:53.083277   62765 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0825 20:04:53.083397   62765 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0825 20:04:53.083439   62765 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0825 20:04:53.092639   62765 docker.go:685] Got preloaded images: -- stdout --
mongo:5.0
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
nanajanashia/k8s-demo-app:v1.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0825 20:04:53.092647   62765 docker.go:615] Images already preloaded, skipping extraction
I0825 20:04:53.092922   62765 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0825 20:04:53.099978   62765 docker.go:685] Got preloaded images: -- stdout --
mongo:5.0
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
nanajanashia/k8s-demo-app:v1.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0825 20:04:53.099988   62765 cache_images.go:84] Images are preloaded, skipping loading
I0825 20:04:53.099992   62765 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0825 20:04:53.100068   62765 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0825 20:04:53.100101   62765 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0825 20:04:53.132207   62765 cni.go:84] Creating CNI manager for ""
I0825 20:04:53.132226   62765 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0825 20:04:53.132243   62765 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0825 20:04:53.132264   62765 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0825 20:04:53.132440   62765 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0825 20:04:53.132550   62765 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0825 20:04:53.136402   62765 binaries.go:44] Found k8s binaries, skipping transfer
I0825 20:04:53.136491   62765 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0825 20:04:53.139658   62765 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0825 20:04:53.146053   62765 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0825 20:04:53.152372   62765 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0825 20:04:53.158614   62765 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0825 20:04:53.160288   62765 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0825 20:04:53.164594   62765 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 20:04:53.196621   62765 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0825 20:04:53.213561   62765 certs.go:68] Setting up /Users/dawid/.minikube/profiles/minikube for IP: 192.168.49.2
I0825 20:04:53.213796   62765 certs.go:194] generating shared ca certs ...
I0825 20:04:53.213824   62765 certs.go:226] acquiring lock for ca certs: {Name:mkf6bfb1b7fc1672c53b2651009924a01d8d152e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 20:04:53.214821   62765 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/dawid/.minikube/ca.key
I0825 20:04:53.215350   62765 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/dawid/.minikube/proxy-client-ca.key
I0825 20:04:53.215368   62765 certs.go:256] generating profile certs ...
I0825 20:04:53.216207   62765 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/dawid/.minikube/profiles/minikube/client.key
I0825 20:04:53.216585   62765 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/dawid/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0825 20:04:53.216847   62765 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/dawid/.minikube/profiles/minikube/proxy-client.key
I0825 20:04:53.217166   62765 certs.go:484] found cert: /Users/dawid/.minikube/certs/ca-key.pem (1675 bytes)
I0825 20:04:53.217233   62765 certs.go:484] found cert: /Users/dawid/.minikube/certs/ca.pem (1074 bytes)
I0825 20:04:53.217285   62765 certs.go:484] found cert: /Users/dawid/.minikube/certs/cert.pem (1115 bytes)
I0825 20:04:53.217331   62765 certs.go:484] found cert: /Users/dawid/.minikube/certs/key.pem (1679 bytes)
I0825 20:04:53.218246   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0825 20:04:53.227257   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0825 20:04:53.236656   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0825 20:04:53.245105   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0825 20:04:53.253715   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0825 20:04:53.263381   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0825 20:04:53.273724   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0825 20:04:53.320982   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0825 20:04:53.332113   62765 ssh_runner.go:362] scp /Users/dawid/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0825 20:04:53.341722   62765 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0825 20:04:53.349570   62765 ssh_runner.go:195] Run: openssl version
I0825 20:04:53.352449   62765 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0825 20:04:53.356208   62765 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0825 20:04:53.358621   62765 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug 25 15:26 /usr/share/ca-certificates/minikubeCA.pem
I0825 20:04:53.358677   62765 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0825 20:04:53.362107   62765 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0825 20:04:53.365745   62765 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0825 20:04:53.367493   62765 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0825 20:04:53.371189   62765 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0825 20:04:53.384799   62765 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0825 20:04:53.390976   62765 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0825 20:04:53.394811   62765 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0825 20:04:53.398192   62765 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0825 20:04:53.401101   62765 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0825 20:04:53.401188   62765 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0825 20:04:53.408022   62765 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0825 20:04:53.420965   62765 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0825 20:04:53.420983   62765 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0825 20:04:53.420990   62765 kubeadm.go:587] restartPrimaryControlPlane start ...
I0825 20:04:53.421148   62765 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0825 20:04:53.424808   62765 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0825 20:04:53.424862   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0825 20:04:53.447752   62765 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /Users/dawid/.kube/config
I0825 20:04:53.447989   62765 kubeconfig.go:62] /Users/dawid/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0825 20:04:53.448347   62765 lock.go:35] WriteFile acquiring /Users/dawid/.kube/config: {Name:mk9d9ac5cf3827c85aa72e45f44aac96ba0c6e88 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 20:04:53.454237   62765 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0825 20:04:53.458439   62765 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0825 20:04:53.458453   62765 kubeadm.go:591] duration metric: took 37.45875ms to restartPrimaryControlPlane
I0825 20:04:53.458456   62765 kubeadm.go:393] duration metric: took 57.359209ms to StartCluster
I0825 20:04:53.458463   62765 settings.go:142] acquiring lock: {Name:mk552e547d824307feed65363661ae2c8500cdd6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 20:04:53.458528   62765 settings.go:150] Updating kubeconfig:  /Users/dawid/.kube/config
I0825 20:04:53.459842   62765 lock.go:35] WriteFile acquiring /Users/dawid/.kube/config: {Name:mk9d9ac5cf3827c85aa72e45f44aac96ba0c6e88 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0825 20:04:53.460060   62765 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0825 20:04:53.460317   62765 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0825 20:04:53.472182   62765 out.go:177] üîé  Verifying Kubernetes components...
I0825 20:04:53.460272   62765 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0825 20:04:53.474082   62765 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0825 20:04:53.474091   62765 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0825 20:04:53.474109   62765 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0825 20:04:53.474111   62765 addons.go:243] addon storage-provisioner should already be in state true
I0825 20:04:53.474116   62765 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0825 20:04:53.474124   62765 host.go:66] Checking if "minikube" exists ...
I0825 20:04:53.474327   62765 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0825 20:04:53.474451   62765 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 20:04:53.474453   62765 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 20:04:53.494548   62765 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0825 20:04:53.493560   62765 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0825 20:04:53.497938   62765 addons.go:243] addon default-storageclass should already be in state true
I0825 20:04:53.497957   62765 host.go:66] Checking if "minikube" exists ...
I0825 20:04:53.498047   62765 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0825 20:04:53.498053   62765 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0825 20:04:53.498091   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:53.498153   62765 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0825 20:04:53.510722   62765 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0825 20:04:53.513546   62765 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0825 20:04:53.513552   62765 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0825 20:04:53.513603   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0825 20:04:53.513714   62765 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58221 SSHKeyPath:/Users/dawid/.minikube/machines/minikube/id_rsa Username:docker}
I0825 20:04:53.517240   62765 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0825 20:04:53.527480   62765 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58221 SSHKeyPath:/Users/dawid/.minikube/machines/minikube/id_rsa Username:docker}
I0825 20:04:53.528937   62765 api_server.go:52] waiting for apiserver process to appear ...
I0825 20:04:53.528991   62765 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0825 20:04:53.601278   62765 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0825 20:04:53.614068   62765 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0825 20:04:53.641892   62765 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0825 20:04:53.641942   62765 retry.go:31] will retry after 224.272865ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0825 20:04:53.645108   62765 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0825 20:04:53.645115   62765 retry.go:31] will retry after 205.528307ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0825 20:04:53.851849   62765 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0825 20:04:53.866864   62765 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0825 20:04:53.935513   62765 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0825 20:04:53.935536   62765 retry.go:31] will retry after 456.702659ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0825 20:04:53.951648   62765 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0825 20:04:53.951688   62765 retry.go:31] will retry after 198.049665ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0825 20:04:54.030274   62765 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0825 20:04:54.040366   62765 api_server.go:72] duration metric: took 580.025209ms to wait for apiserver process to appear ...
I0825 20:04:54.040380   62765 api_server.go:88] waiting for apiserver healthz status ...
I0825 20:04:54.040767   62765 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58225/healthz ...
I0825 20:04:54.041727   62765 api_server.go:269] stopped: https://127.0.0.1:58225/healthz: Get "https://127.0.0.1:58225/healthz": EOF
I0825 20:04:54.150082   62765 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0825 20:04:54.393228   62765 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0825 20:04:54.541195   62765 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58225/healthz ...
I0825 20:04:55.851445   62765 api_server.go:279] https://127.0.0.1:58225/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0825 20:04:55.851464   62765 api_server.go:103] status: https://127.0.0.1:58225/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0825 20:04:55.851477   62765 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58225/healthz ...
I0825 20:04:55.924390   62765 api_server.go:279] https://127.0.0.1:58225/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0825 20:04:55.924420   62765 api_server.go:103] status: https://127.0.0.1:58225/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0825 20:04:56.041463   62765 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58225/healthz ...
I0825 20:04:56.044946   62765 api_server.go:279] https://127.0.0.1:58225/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0825 20:04:56.044969   62765 api_server.go:103] status: https://127.0.0.1:58225/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0825 20:04:56.183972   62765 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.033827s)
I0825 20:04:56.184047   62765 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.79070925s)
I0825 20:04:56.222537   62765 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0825 20:04:56.225458   62765 addons.go:505] duration metric: took 2.765271375s for enable addons: enabled=[storage-provisioner default-storageclass]
I0825 20:04:56.541606   62765 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58225/healthz ...
I0825 20:04:56.547236   62765 api_server.go:279] https://127.0.0.1:58225/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0825 20:04:56.547249   62765 api_server.go:103] status: https://127.0.0.1:58225/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0825 20:04:57.040575   62765 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58225/healthz ...
I0825 20:04:57.049829   62765 api_server.go:279] https://127.0.0.1:58225/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0825 20:04:57.049850   62765 api_server.go:103] status: https://127.0.0.1:58225/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0825 20:04:57.541746   62765 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58225/healthz ...
I0825 20:04:57.546497   62765 api_server.go:279] https://127.0.0.1:58225/healthz returned 200:
ok
I0825 20:04:57.547332   62765 api_server.go:141] control plane version: v1.30.0
I0825 20:04:57.547352   62765 api_server.go:131] duration metric: took 3.506918416s to wait for apiserver health ...
I0825 20:04:57.547370   62765 system_pods.go:43] waiting for kube-system pods to appear ...
I0825 20:04:57.555382   62765 system_pods.go:59] 7 kube-system pods found
I0825 20:04:57.555399   62765 system_pods.go:61] "coredns-7db6d8ff4d-xhcfz" [b49ecabd-7eb9-465b-8a10-293eec3ffb85] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0825 20:04:57.555408   62765 system_pods.go:61] "etcd-minikube" [ab72e0b9-deab-4c94-9ddd-47d6de0ca566] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0825 20:04:57.555416   62765 system_pods.go:61] "kube-apiserver-minikube" [0d7835f3-6a6b-4793-8eff-d98806e77147] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0825 20:04:57.555423   62765 system_pods.go:61] "kube-controller-manager-minikube" [197ea6d8-676d-4ae5-8902-0eb453fc1c63] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0825 20:04:57.555428   62765 system_pods.go:61] "kube-proxy-cnjjr" [99d5cae6-3e79-4730-a0f2-ee27df736e36] Running
I0825 20:04:57.555434   62765 system_pods.go:61] "kube-scheduler-minikube" [15075dab-3275-4144-80a1-916f6e1b7b9e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0825 20:04:57.555440   62765 system_pods.go:61] "storage-provisioner" [49b118c1-9570-4b64-a385-5d825b70adec] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0825 20:04:57.555445   62765 system_pods.go:74] duration metric: took 8.07ms to wait for pod list to return data ...
I0825 20:04:57.555453   62765 kubeadm.go:576] duration metric: took 4.095071417s to wait for: map[apiserver:true system_pods:true]
I0825 20:04:57.555464   62765 node_conditions.go:102] verifying NodePressure condition ...
I0825 20:04:57.557164   62765 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0825 20:04:57.557172   62765 node_conditions.go:123] node cpu capacity is 8
I0825 20:04:57.557180   62765 node_conditions.go:105] duration metric: took 1.712792ms to run NodePressure ...
I0825 20:04:57.557187   62765 start.go:240] waiting for startup goroutines ...
I0825 20:04:57.557193   62765 start.go:245] waiting for cluster config update ...
I0825 20:04:57.557201   62765 start.go:254] writing updated cluster config ...
I0825 20:04:57.557736   62765 ssh_runner.go:195] Run: rm -f paused
I0825 20:04:57.606612   62765 start.go:600] kubectl: 1.30.2, cluster: 1.30.0 (minor skew: 0)
I0825 20:04:57.612221   62765 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 25 18:04:52 minikube systemd[1]: docker.service: Deactivated successfully.
Aug 25 18:04:52 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 25 18:04:52 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.357276174Z" level=info msg="Starting up"
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.366819549Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.375972466Z" level=info msg="Loading containers: start."
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.449750882Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.461432882Z" level=info msg="Loading containers: done."
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.484473466Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.484526174Z" level=info msg="Daemon has completed initialization"
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.498430507Z" level=info msg="Processing signal 'terminated'"
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.499943257Z" level=info msg="API listen on [::]:2376"
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.499962424Z" level=info msg="API listen on /var/run/docker.sock"
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.500435174Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Aug 25 18:04:52 minikube dockerd[635]: time="2024-08-25T18:04:52.500613966Z" level=info msg="Daemon shutdown complete"
Aug 25 18:04:52 minikube systemd[1]: docker.service: Deactivated successfully.
Aug 25 18:04:52 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 25 18:04:52 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.552597966Z" level=info msg="Starting up"
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.560880383Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.567222716Z" level=info msg="Loading containers: start."
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.626632383Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.636307758Z" level=info msg="Loading containers: done."
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.657402174Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.657446966Z" level=info msg="Daemon has completed initialization"
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.671594799Z" level=info msg="API listen on /var/run/docker.sock"
Aug 25 18:04:52 minikube dockerd[874]: time="2024-08-25T18:04:52.671614174Z" level=info msg="API listen on [::]:2376"
Aug 25 18:04:52 minikube systemd[1]: Started Docker Application Container Engine.
Aug 25 18:04:52 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Start docker client with request timeout 0s"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Loaded network plugin cni"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 25 18:04:52 minikube cri-dockerd[1123]: time="2024-08-25T18:04:52Z" level=info msg="Start cri-dockerd grpc backend"
Aug 25 18:04:52 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-xhcfz_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c25a25f88ab7f77f45dadc63f959cf00dff2a8f05f49aa16fa9e54171e93aeaf\""
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-xhcfz_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1659260ac40b6572fd31033eae83ec151bfa80b9a115bc26da24688fcff8512d\""
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-bd7557f5c-zr866_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"88ead41bcc1e96cbcc32ee275140b4f7d3a04c12417667298413d30d066e902f\""
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5cd59c6ff5-6vndn_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8cee198f275d7bd43e811cca1955b4b992334b3903e63bf50b4510fecc9a6fdb\""
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"af6f230040e3d56e280661df132ee72bce82cae1423662b06e2136ff1892da42\". Proceed without further sandbox information."
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"e3554aab77851cfd33c5450c678062bca830689930cf7d6f3e2d1e61d58acc01\". Proceed without further sandbox information."
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"693b7d9dbfbeabfca3edd3f1272cd48faae3325af98fd776bac50eda96015e5c\". Proceed without further sandbox information."
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a00ce0a5d79babc64001dae3df30e2e65ade4c1b314a7eadd3c2dbc2ec138374\". Proceed without further sandbox information."
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b1f8b0ecd038cdba74dba3b859d9034e49c651cf81bff17123dffe16067cc48/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/96d87294aaacbd1823650f7b470c74a8d6f843e9cb1664c5530e7952d17e2532/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d26b641d2434fdc39fe80c5fb39fbcc10b7209343a6185e69450847e04480ab6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 18:04:53 minikube cri-dockerd[1123]: time="2024-08-25T18:04:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0f1bb023ec6a3209979d7b5420dbc92f6bb527329347447c1c151176e1d15b49/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 18:04:54 minikube cri-dockerd[1123]: time="2024-08-25T18:04:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-xhcfz_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c25a25f88ab7f77f45dadc63f959cf00dff2a8f05f49aa16fa9e54171e93aeaf\""
Aug 25 18:04:55 minikube cri-dockerd[1123]: time="2024-08-25T18:04:55Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 25 18:04:56 minikube cri-dockerd[1123]: time="2024-08-25T18:04:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4ca4d66dd35559a7c2eb867123d46833c27a7adc1d8c55f73f1ec84ed4671d7e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 18:04:56 minikube cri-dockerd[1123]: time="2024-08-25T18:04:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5b9458738b2d24445b18aa4b819892b5951c4617a8dc560ac5c8ceb0cc19ccbb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 18:04:56 minikube cri-dockerd[1123]: time="2024-08-25T18:04:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09355c699375cd613c5f46e1680af16694ff66ced08907a51d8fce4ebed0c78b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 25 18:04:56 minikube cri-dockerd[1123]: time="2024-08-25T18:04:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b5a0629297b6425f80787c1c6f2ea26297df831dc2f5e8e25d4628a48427709b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 25 18:04:56 minikube cri-dockerd[1123]: time="2024-08-25T18:04:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5e44532faa9e31dfe1a524125491a4b36845ce857aa94da5274676ff806537fd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 25 18:04:57 minikube dockerd[874]: time="2024-08-25T18:04:57.050699885Z" level=info msg="ignoring event" container=c4a40873c468b9dd8912f66f495e6ceebd1976183ad5857264b74f25d31cdf9b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
529f8bf87fa24       ba04bb24b9575       2 minutes ago       Running             storage-provisioner       5                   4ca4d66dd3555       storage-provisioner
0895fa71aabf2       2437cf7621777       2 minutes ago       Running             coredns                   2                   09355c699375c       coredns-7db6d8ff4d-xhcfz
9ba0cf8ab27bb       cb7eac0b42cc1       2 minutes ago       Running             kube-proxy                2                   5b9458738b2d2       kube-proxy-cnjjr
c4a40873c468b       ba04bb24b9575       2 minutes ago       Exited              storage-provisioner       4                   4ca4d66dd3555       storage-provisioner
7017c40e374bf       547adae34140b       2 minutes ago       Running             kube-scheduler            2                   0f1bb023ec6a3       kube-scheduler-minikube
dcbb023948505       68feac521c0f1       2 minutes ago       Running             kube-controller-manager   2                   d26b641d2434f       kube-controller-manager-minikube
e891a0d715e1f       014faa467e297       2 minutes ago       Running             etcd                      2                   96d87294aaacb       etcd-minikube
b554890d61afd       181f57fd3cdb7       2 minutes ago       Running             kube-apiserver            2                   9b1f8b0ecd038       kube-apiserver-minikube
b004223cc5c15       2437cf7621777       About an hour ago   Exited              coredns                   1                   c25a25f88ab7f       coredns-7db6d8ff4d-xhcfz
d4d555f834357       cb7eac0b42cc1       About an hour ago   Exited              kube-proxy                1                   8d663c8d36d7c       kube-proxy-cnjjr
f9c6fcad65b87       181f57fd3cdb7       About an hour ago   Exited              kube-apiserver            1                   8788c165e1a6e       kube-apiserver-minikube
027877f4b0254       547adae34140b       About an hour ago   Exited              kube-scheduler            1                   9b39203bf6b25       kube-scheduler-minikube
1c7df9d5efbc0       68feac521c0f1       About an hour ago   Exited              kube-controller-manager   1                   15c6a7aa8baf0       kube-controller-manager-minikube
6fe89d547ef3d       014faa467e297       About an hour ago   Exited              etcd                      1                   5ffd5cebdb167       etcd-minikube


==> coredns [0895fa71aabf] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:55992 - 54639 "HINFO IN 2728616454181731551.7447812524293861484. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.10870475s


==> coredns [b004223cc5c1] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:35890 - 43804 "HINFO IN 7007410914183532341.2510394544422399861. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.1680775s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_08_25T17_26_21_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 25 Aug 2024 15:26:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 25 Aug 2024 18:07:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 25 Aug 2024 18:04:55 +0000   Sun, 25 Aug 2024 15:26:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 25 Aug 2024 18:04:55 +0000   Sun, 25 Aug 2024 15:26:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 25 Aug 2024 18:04:55 +0000   Sun, 25 Aug 2024 15:26:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 25 Aug 2024 18:04:55 +0000   Sun, 25 Aug 2024 15:26:18 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8027168Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8027168Ki
  pods:               110
System Info:
  Machine ID:                 09d09d71a4744c21ad75d1628f51842c
  System UUID:                09d09d71a4744c21ad75d1628f51842c
  Boot ID:                    ae372a33-0e4a-4282-8eb3-c721544f56c2
  Kernel Version:             6.10.0-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     mongo-deployment-5cd59c6ff5-6vndn    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         102m
  default                     webapp-deployment-bd7557f5c-zr866    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         98m
  kube-system                 coredns-7db6d8ff4d-xhcfz             100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     160m
  kube-system                 etcd-minikube                        100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         161m
  kube-system                 kube-apiserver-minikube              250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         161m
  kube-system                 kube-controller-manager-minikube     200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         161m
  kube-system                 kube-proxy-cnjjr                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         160m
  kube-system                 kube-scheduler-minikube              100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         161m
  kube-system                 storage-provisioner                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         161m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 2m30s                  kube-proxy       
  Normal  Starting                 2m34s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  2m34s (x8 over 2m34s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m34s (x8 over 2m34s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m34s (x7 over 2m34s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  2m34s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           2m19s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Aug25 16:46] netlink: 'init': attribute type 4 has an invalid length.
[  +0.018139] fakeowner: loading out-of-tree module taints kernel.
[ +35.578804] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000021] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.376654] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000004] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.519342] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000131] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Aug25 16:58] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000031] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.


==> etcd [6fe89d547ef3] <==
{"level":"info","ts":"2024-08-25T16:54:04.184947Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-08-25T16:54:04.184953Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-08-25T16:54:04.184975Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-08-25T16:54:04.184986Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-08-25T16:54:04.185952Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-08-25T16:54:04.186104Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-25T16:54:04.186362Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-25T16:54:04.186473Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-08-25T16:54:04.186503Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-08-25T16:54:04.188475Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-08-25T16:54:04.188647Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-08-25T17:04:04.211551Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4632}
{"level":"info","ts":"2024-08-25T17:04:04.240067Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4632,"took":"24.4355ms","hash":231575245,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1351680,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-08-25T17:04:04.240159Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":231575245,"revision":4632,"compact-revision":3808}
{"level":"info","ts":"2024-08-25T17:09:04.23574Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4874}
{"level":"info","ts":"2024-08-25T17:09:04.237848Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4874,"took":"1.917708ms","hash":4119677078,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1613824,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:09:04.23788Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4119677078,"revision":4874,"compact-revision":4632}
{"level":"info","ts":"2024-08-25T17:14:04.245975Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5117}
{"level":"info","ts":"2024-08-25T17:14:04.25129Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5117,"took":"4.400375ms","hash":1861577883,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1601536,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:14:04.251385Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1861577883,"revision":5117,"compact-revision":4874}
{"level":"info","ts":"2024-08-25T17:19:04.253797Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5358}
{"level":"info","ts":"2024-08-25T17:19:04.256165Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5358,"took":"1.998125ms","hash":3013690314,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1597440,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:19:04.256204Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3013690314,"revision":5358,"compact-revision":5117}
{"level":"info","ts":"2024-08-25T17:24:04.259856Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5600}
{"level":"info","ts":"2024-08-25T17:24:04.265072Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5600,"took":"4.623792ms","hash":728708869,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1605632,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:24:04.265167Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":728708869,"revision":5600,"compact-revision":5358}
{"level":"info","ts":"2024-08-25T17:29:04.28377Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5842}
{"level":"info","ts":"2024-08-25T17:29:04.300583Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5842,"took":"14.081542ms","hash":3219741424,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1605632,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:29:04.300623Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3219741424,"revision":5842,"compact-revision":5600}
{"level":"info","ts":"2024-08-25T17:34:04.298075Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6083}
{"level":"info","ts":"2024-08-25T17:34:04.299468Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6083,"took":"1.190666ms","hash":2844083584,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1626112,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:34:04.299497Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2844083584,"revision":6083,"compact-revision":5842}
{"level":"info","ts":"2024-08-25T17:39:04.305268Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6325}
{"level":"info","ts":"2024-08-25T17:39:04.307599Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6325,"took":"2.0415ms","hash":4023552226,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1581056,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:39:04.307633Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4023552226,"revision":6325,"compact-revision":6083}
{"level":"info","ts":"2024-08-25T17:44:04.31416Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6567}
{"level":"info","ts":"2024-08-25T17:44:04.316304Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6567,"took":"1.807125ms","hash":3958585026,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:44:04.316337Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3958585026,"revision":6567,"compact-revision":6325}
{"level":"info","ts":"2024-08-25T17:49:04.319909Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6810}
{"level":"info","ts":"2024-08-25T17:49:04.322487Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6810,"took":"2.222708ms","hash":2161379277,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1622016,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:49:04.322519Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2161379277,"revision":6810,"compact-revision":6567}
{"level":"info","ts":"2024-08-25T17:54:04.326283Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7052}
{"level":"info","ts":"2024-08-25T17:54:04.329424Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7052,"took":"2.700333ms","hash":1426066695,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1605632,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:54:04.329477Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1426066695,"revision":7052,"compact-revision":6810}
{"level":"info","ts":"2024-08-25T17:59:04.372046Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7293}
{"level":"info","ts":"2024-08-25T17:59:04.375442Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7293,"took":"2.972375ms","hash":1043135831,"current-db-size-bytes":2433024,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1613824,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-25T17:59:04.375506Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1043135831,"revision":7293,"compact-revision":7052}
{"level":"info","ts":"2024-08-25T18:04:04.384855Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7545}
{"level":"info","ts":"2024-08-25T18:04:04.391503Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7545,"took":"6.226667ms","hash":1404557940,"current-db-size-bytes":2564096,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1794048,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-08-25T18:04:04.391576Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1404557940,"revision":7545,"compact-revision":7293}
{"level":"info","ts":"2024-08-25T18:04:23.884022Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-08-25T18:04:23.884287Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-08-25T18:04:23.884535Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-25T18:04:23.884605Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-25T18:04:23.884729Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-25T18:04:23.884826Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"info","ts":"2024-08-25T18:04:23.953292Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-08-25T18:04:23.95676Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-25T18:04:23.956924Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-25T18:04:23.956959Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [e891a0d715e1] <==
{"level":"warn","ts":"2024-08-25T18:04:53.921613Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-25T18:04:53.921859Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-08-25T18:04:53.921918Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-08-25T18:04:53.921944Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-08-25T18:04:53.921953Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-08-25T18:04:53.922009Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-25T18:04:53.924102Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-08-25T18:04:53.924396Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"arm64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-08-25T18:04:53.927074Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.43675ms"}
{"level":"info","ts":"2024-08-25T18:04:54.031373Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-08-25T18:04:54.053818Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":9679}
{"level":"info","ts":"2024-08-25T18:04:54.055206Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-08-25T18:04:54.055249Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2024-08-25T18:04:54.055257Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 9679, applied: 0, lastindex: 9679, lastterm: 3]"}
{"level":"warn","ts":"2024-08-25T18:04:54.055888Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-08-25T18:04:54.056214Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":7545}
{"level":"info","ts":"2024-08-25T18:04:54.056984Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":7805}
{"level":"info","ts":"2024-08-25T18:04:54.058382Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-08-25T18:04:54.060136Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-08-25T18:04:54.060321Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-08-25T18:04:54.060348Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-08-25T18:04:54.061345Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-08-25T18:04:54.061484Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-25T18:04:54.061517Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-25T18:04:54.061521Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-08-25T18:04:54.061525Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-08-25T18:04:54.061628Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-25T18:04:54.061633Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-25T18:04:54.061645Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-08-25T18:04:54.061654Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-08-25T18:04:54.118102Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-08-25T18:04:54.118185Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-08-25T18:04:54.118278Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-25T18:04:54.118295Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-08-25T18:04:55.256468Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2024-08-25T18:04:55.256668Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2024-08-25T18:04:55.256706Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-08-25T18:04:55.256736Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2024-08-25T18:04:55.256748Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-08-25T18:04:55.25677Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2024-08-25T18:04:55.256919Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-08-25T18:04:55.260929Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-25T18:04:55.26097Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-08-25T18:04:55.260925Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-08-25T18:04:55.261347Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-08-25T18:04:55.261376Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-08-25T18:04:55.263831Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-08-25T18:04:55.263831Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}


==> kernel <==
 18:07:28 up  1:21,  0 users,  load average: 3.62, 3.81, 3.97
Linux minikube 6.10.0-linuxkit #1 SMP Wed Jul 17 10:51:09 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [b554890d61af] <==
W0825 18:04:55.574143       1 genericapiserver.go:733] Skipping API apps/v1beta1 because it has no resources.
I0825 18:04:55.574774       1 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0825 18:04:55.574781       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0825 18:04:55.574792       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0825 18:04:55.574972       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0825 18:04:55.574980       1 genericapiserver.go:733] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0825 18:04:55.591795       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0825 18:04:55.591809       1 genericapiserver.go:733] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0825 18:04:55.828117       1 secure_serving.go:213] Serving securely on [::]:8443
I0825 18:04:55.828212       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0825 18:04:55.828323       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0825 18:04:55.828369       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0825 18:04:55.828484       1 available_controller.go:423] Starting AvailableConditionController
I0825 18:04:55.828505       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0825 18:04:55.828525       1 aggregator.go:163] waiting for initial CRD sync...
I0825 18:04:55.828607       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0825 18:04:55.828981       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0825 18:04:55.829083       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0825 18:04:55.829089       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0825 18:04:55.829100       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0825 18:04:55.829192       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0825 18:04:55.829225       1 controller.go:116] Starting legacy_token_tracking_controller
I0825 18:04:55.829230       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0825 18:04:55.829238       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0825 18:04:55.829240       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0825 18:04:55.829252       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0825 18:04:55.829276       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0825 18:04:55.829282       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0825 18:04:55.829363       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0825 18:04:55.829401       1 controller.go:78] Starting OpenAPI AggregationController
I0825 18:04:55.829442       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0825 18:04:55.829473       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0825 18:04:55.829476       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0825 18:04:55.829760       1 controller.go:139] Starting OpenAPI controller
I0825 18:04:55.829775       1 controller.go:87] Starting OpenAPI V3 controller
I0825 18:04:55.829808       1 naming_controller.go:291] Starting NamingConditionController
I0825 18:04:55.829827       1 establishing_controller.go:76] Starting EstablishingController
I0825 18:04:55.829840       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0825 18:04:55.829851       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0825 18:04:55.829856       1 crd_finalizer.go:266] Starting CRDFinalizer
I0825 18:04:55.918627       1 shared_informer.go:320] Caches are synced for node_authorizer
I0825 18:04:55.919669       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0825 18:04:55.919695       1 policy_source.go:224] refreshing policies
I0825 18:04:55.928892       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0825 18:04:55.929031       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0825 18:04:55.929040       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0825 18:04:55.929265       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0825 18:04:55.929978       1 shared_informer.go:320] Caches are synced for configmaps
I0825 18:04:55.930000       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0825 18:04:55.930587       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0825 18:04:55.930603       1 aggregator.go:165] initial CRD sync complete...
I0825 18:04:55.930611       1 autoregister_controller.go:141] Starting autoregister controller
I0825 18:04:55.930614       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0825 18:04:55.930616       1 cache.go:39] Caches are synced for autoregister controller
I0825 18:04:55.932265       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0825 18:04:55.933465       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0825 18:04:55.944136       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0825 18:04:56.833129       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0825 18:05:08.573689       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0825 18:05:08.576958       1 controller.go:615] quota admission added evaluator for: endpoints


==> kube-apiserver [f9c6fcad65b8] <==
W0825 18:04:29.586790       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:29.630360       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:29.704338       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:29.704338       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:29.740541       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:29.755093       1 logging.go:59] [core] [Channel #520 SubChannel #521] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.069625       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.168948       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.171502       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.287619       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.345478       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.475937       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.515088       1 logging.go:59] [core] [Channel #520 SubChannel #521] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.536077       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.579245       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.582072       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.582138       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.594152       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.648571       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.680899       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.788591       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.949997       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.956869       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:32.959974       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.106782       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.116977       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.123814       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.129061       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.135992       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.151213       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.162853       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.179128       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.187018       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.192768       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.242389       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.284430       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.289889       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.324871       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.333558       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.392838       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.402670       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.411023       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.457840       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.460832       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.502751       1 logging.go:59] [core] [Channel #1 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.539699       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.572295       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.582353       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.590959       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.593668       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.610257       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.647541       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.719347       1 logging.go:59] [core] [Channel #2 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.749174       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.751756       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.755114       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.775969       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.831991       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.869501       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0825 18:04:33.956045       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [1c7df9d5efbc] <==
E0825 16:54:16.846954       1 core.go:105] "Failed to start service controller" err="WARNING: no cloud provider provided, services of type LoadBalancer will fail" logger="service-lb-controller"
I0825 16:54:16.846966       1 controllermanager.go:737] "Warning: skipping controller" controller="service-lb-controller"
I0825 16:54:16.897346       1 controllermanager.go:759] "Started controller" controller="persistentvolume-expander-controller"
I0825 16:54:16.898908       1 expand_controller.go:329] "Starting expand controller" logger="persistentvolume-expander-controller"
I0825 16:54:16.898923       1 shared_informer.go:313] Waiting for caches to sync for expand
I0825 16:54:16.901314       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0825 16:54:16.907833       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0825 16:54:16.908314       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0825 16:54:16.916789       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0825 16:54:16.919012       1 shared_informer.go:320] Caches are synced for TTL after finished
I0825 16:54:16.922953       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0825 16:54:16.924988       1 shared_informer.go:320] Caches are synced for PVC protection
I0825 16:54:16.925010       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0825 16:54:16.926996       1 shared_informer.go:320] Caches are synced for PV protection
I0825 16:54:16.937366       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0825 16:54:16.940156       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0825 16:54:16.940295       1 shared_informer.go:320] Caches are synced for endpoint
I0825 16:54:16.940414       1 shared_informer.go:320] Caches are synced for persistent volume
I0825 16:54:16.940559       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0825 16:54:16.941235       1 shared_informer.go:320] Caches are synced for service account
I0825 16:54:16.952936       1 shared_informer.go:320] Caches are synced for node
I0825 16:54:16.952958       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0825 16:54:16.952967       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0825 16:54:16.952970       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0825 16:54:16.952973       1 shared_informer.go:320] Caches are synced for cidrallocator
I0825 16:54:16.954943       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0825 16:54:16.954963       1 shared_informer.go:320] Caches are synced for ephemeral
I0825 16:54:16.956942       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0825 16:54:16.956993       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-bd7557f5c" duration="20.833¬µs"
I0825 16:54:16.956993       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5cd59c6ff5" duration="27.166¬µs"
I0825 16:54:16.957046       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="58.333¬µs"
I0825 16:54:16.959106       1 shared_informer.go:320] Caches are synced for crt configmap
I0825 16:54:16.959890       1 shared_informer.go:320] Caches are synced for attach detach
I0825 16:54:16.960929       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0825 16:54:16.961898       1 shared_informer.go:320] Caches are synced for ReplicationController
I0825 16:54:16.969995       1 shared_informer.go:320] Caches are synced for job
I0825 16:54:16.971948       1 shared_informer.go:320] Caches are synced for GC
I0825 16:54:16.972872       1 shared_informer.go:320] Caches are synced for TTL
I0825 16:54:16.972906       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0825 16:54:16.972911       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0825 16:54:16.972946       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0825 16:54:16.972971       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0825 16:54:16.976179       1 shared_informer.go:320] Caches are synced for namespace
I0825 16:54:16.977939       1 shared_informer.go:320] Caches are synced for HPA
I0825 16:54:16.979933       1 shared_informer.go:320] Caches are synced for cronjob
I0825 16:54:17.000930       1 shared_informer.go:320] Caches are synced for expand
I0825 16:54:17.122902       1 shared_informer.go:320] Caches are synced for stateful set
I0825 16:54:17.155819       1 shared_informer.go:320] Caches are synced for deployment
I0825 16:54:17.167934       1 shared_informer.go:320] Caches are synced for resource quota
I0825 16:54:17.170893       1 shared_informer.go:320] Caches are synced for daemon sets
I0825 16:54:17.197876       1 shared_informer.go:320] Caches are synced for disruption
I0825 16:54:17.201879       1 shared_informer.go:320] Caches are synced for resource quota
I0825 16:54:17.216947       1 shared_informer.go:320] Caches are synced for taint
I0825 16:54:17.216999       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0825 16:54:17.217054       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0825 16:54:17.217096       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0825 16:54:17.608947       1 shared_informer.go:320] Caches are synced for garbage collector
I0825 16:54:17.642898       1 shared_informer.go:320] Caches are synced for garbage collector
I0825 16:54:17.642924       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0825 17:54:16.797886       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-4dhfp" approvedExpiration="1h0m0s"


==> kube-controller-manager [dcbb02394850] <==
I0825 18:05:08.531362       1 node_lifecycle_controller.go:459] "Sending events to api server" logger="node-lifecycle-controller"
I0825 18:05:08.531377       1 node_lifecycle_controller.go:470] "Starting node controller" logger="node-lifecycle-controller"
I0825 18:05:08.531391       1 shared_informer.go:313] Waiting for caches to sync for taint
I0825 18:05:08.532233       1 controllermanager.go:759] "Started controller" controller="daemonset-controller"
I0825 18:05:08.533313       1 daemon_controller.go:289] "Starting daemon sets controller" logger="daemonset-controller"
I0825 18:05:08.533320       1 shared_informer.go:313] Waiting for caches to sync for daemon sets
I0825 18:05:08.535671       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0825 18:05:08.541716       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0825 18:05:08.545582       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0825 18:05:08.553538       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0825 18:05:08.554394       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0825 18:05:08.554427       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0825 18:05:08.554439       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0825 18:05:08.554535       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0825 18:05:08.554638       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0825 18:05:08.559474       1 shared_informer.go:320] Caches are synced for namespace
I0825 18:05:08.562391       1 shared_informer.go:320] Caches are synced for PVC protection
I0825 18:05:08.564404       1 shared_informer.go:320] Caches are synced for service account
I0825 18:05:08.568455       1 shared_informer.go:320] Caches are synced for crt configmap
I0825 18:05:08.568483       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0825 18:05:08.569372       1 shared_informer.go:320] Caches are synced for ReplicationController
I0825 18:05:08.569991       1 shared_informer.go:320] Caches are synced for GC
I0825 18:05:08.571386       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0825 18:05:08.571539       1 shared_informer.go:320] Caches are synced for endpoint
I0825 18:05:08.571569       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0825 18:05:08.571670       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-5cd59c6ff5" duration="79.459¬µs"
I0825 18:05:08.571730       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-bd7557f5c" duration="12.375¬µs"
I0825 18:05:08.571913       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="180.917¬µs"
I0825 18:05:08.578015       1 shared_informer.go:320] Caches are synced for TTL
I0825 18:05:08.578066       1 shared_informer.go:320] Caches are synced for HPA
I0825 18:05:08.578380       1 shared_informer.go:320] Caches are synced for persistent volume
I0825 18:05:08.579355       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0825 18:05:08.620922       1 shared_informer.go:320] Caches are synced for node
I0825 18:05:08.620967       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0825 18:05:08.620980       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0825 18:05:08.620985       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0825 18:05:08.620988       1 shared_informer.go:320] Caches are synced for cidrallocator
I0825 18:05:08.622543       1 shared_informer.go:320] Caches are synced for expand
I0825 18:05:08.624416       1 shared_informer.go:320] Caches are synced for PV protection
I0825 18:05:08.624462       1 shared_informer.go:320] Caches are synced for ephemeral
I0825 18:05:08.626405       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0825 18:05:08.628430       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0825 18:05:08.628512       1 shared_informer.go:320] Caches are synced for stateful set
I0825 18:05:08.630592       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0825 18:05:08.632423       1 shared_informer.go:320] Caches are synced for taint
I0825 18:05:08.632480       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0825 18:05:08.632528       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0825 18:05:08.632561       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0825 18:05:08.634248       1 shared_informer.go:320] Caches are synced for daemon sets
I0825 18:05:08.664416       1 shared_informer.go:320] Caches are synced for deployment
I0825 18:05:08.667418       1 shared_informer.go:320] Caches are synced for disruption
I0825 18:05:08.667444       1 shared_informer.go:320] Caches are synced for TTL after finished
I0825 18:05:08.674412       1 shared_informer.go:320] Caches are synced for job
I0825 18:05:08.703632       1 shared_informer.go:320] Caches are synced for cronjob
I0825 18:05:08.805286       1 shared_informer.go:320] Caches are synced for attach detach
I0825 18:05:08.836510       1 shared_informer.go:320] Caches are synced for resource quota
I0825 18:05:08.861391       1 shared_informer.go:320] Caches are synced for resource quota
I0825 18:05:09.246264       1 shared_informer.go:320] Caches are synced for garbage collector
I0825 18:05:09.261446       1 shared_informer.go:320] Caches are synced for garbage collector
I0825 18:05:09.261461       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [9ba0cf8ab27b] <==
I0825 18:04:57.142632       1 server_linux.go:69] "Using iptables proxy"
I0825 18:04:57.150076       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0825 18:04:57.161814       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0825 18:04:57.161838       1 server_linux.go:165] "Using iptables Proxier"
I0825 18:04:57.162845       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0825 18:04:57.162919       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0825 18:04:57.163000       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0825 18:04:57.163265       1 server.go:872] "Version info" version="v1.30.0"
I0825 18:04:57.163300       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0825 18:04:57.166090       1 config.go:192] "Starting service config controller"
I0825 18:04:57.166154       1 shared_informer.go:313] Waiting for caches to sync for service config
I0825 18:04:57.166213       1 config.go:101] "Starting endpoint slice config controller"
I0825 18:04:57.166226       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0825 18:04:57.166806       1 config.go:319] "Starting node config controller"
I0825 18:04:57.166817       1 shared_informer.go:313] Waiting for caches to sync for node config
I0825 18:04:57.267023       1 shared_informer.go:320] Caches are synced for node config
I0825 18:04:57.267058       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0825 18:04:57.267142       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [d4d555f83435] <==
I0825 16:54:06.225607       1 server_linux.go:69] "Using iptables proxy"
I0825 16:54:06.238459       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0825 16:54:06.265328       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0825 16:54:06.265366       1 server_linux.go:165] "Using iptables Proxier"
I0825 16:54:06.266156       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0825 16:54:06.266167       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0825 16:54:06.266461       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0825 16:54:06.266831       1 server.go:872] "Version info" version="v1.30.0"
I0825 16:54:06.266848       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0825 16:54:06.270787       1 config.go:192] "Starting service config controller"
I0825 16:54:06.270794       1 config.go:101] "Starting endpoint slice config controller"
I0825 16:54:06.271124       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0825 16:54:06.271228       1 config.go:319] "Starting node config controller"
I0825 16:54:06.271283       1 shared_informer.go:313] Waiting for caches to sync for service config
I0825 16:54:06.271349       1 shared_informer.go:313] Waiting for caches to sync for node config
I0825 16:54:06.372973       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0825 16:54:06.373010       1 shared_informer.go:320] Caches are synced for node config
I0825 16:54:06.372972       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [027877f4b025] <==
I0825 16:54:02.708616       1 serving.go:380] Generated self-signed cert in-memory
W0825 16:54:04.700784       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0825 16:54:04.702354       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0825 16:54:04.702380       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0825 16:54:04.702388       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0825 16:54:04.776382       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0825 16:54:04.776405       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0825 16:54:04.778612       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0825 16:54:04.778703       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0825 16:54:04.778732       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0825 16:54:04.779931       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0825 16:54:04.880600       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0825 18:04:23.884440       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [7017c40e374b] <==
I0825 18:04:54.361464       1 serving.go:380] Generated self-signed cert in-memory
W0825 18:04:55.840034       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0825 18:04:55.840061       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0825 18:04:55.840070       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0825 18:04:55.840074       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0825 18:04:55.853053       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0825 18:04:55.853069       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0825 18:04:55.854469       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0825 18:04:55.854561       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0825 18:04:55.854578       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0825 18:04:55.854593       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0825 18:04:55.956027       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Aug 25 18:04:56 minikube kubelet[1338]: E0825 18:04:56.438925    1338 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Aug 25 18:04:56 minikube kubelet[1338]: I0825 18:04:56.447990    1338 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/99d5cae6-3e79-4730-a0f2-ee27df736e36-xtables-lock\") pod \"kube-proxy-cnjjr\" (UID: \"99d5cae6-3e79-4730-a0f2-ee27df736e36\") " pod="kube-system/kube-proxy-cnjjr"
Aug 25 18:04:56 minikube kubelet[1338]: I0825 18:04:56.448056    1338 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/49b118c1-9570-4b64-a385-5d825b70adec-tmp\") pod \"storage-provisioner\" (UID: \"49b118c1-9570-4b64-a385-5d825b70adec\") " pod="kube-system/storage-provisioner"
Aug 25 18:04:56 minikube kubelet[1338]: I0825 18:04:56.448085    1338 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/99d5cae6-3e79-4730-a0f2-ee27df736e36-lib-modules\") pod \"kube-proxy-cnjjr\" (UID: \"99d5cae6-3e79-4730-a0f2-ee27df736e36\") " pod="kube-system/kube-proxy-cnjjr"
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:56.952773    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:56.952821    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:57.026703    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:57.026751    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:57.445605    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:57.445643    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:57.460567    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:57.460590    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:04:57 minikube kubelet[1338]: I0825 18:04:57.463432    1338 scope.go:117] "RemoveContainer" containerID="59cebc4dfc0d03275bb96dd80cfe0169ebcbec8e2ed23e6e6a06fed1bf63cc12"
Aug 25 18:04:57 minikube kubelet[1338]: I0825 18:04:57.463590    1338 scope.go:117] "RemoveContainer" containerID="c4a40873c468b9dd8912f66f495e6ceebd1976183ad5857264b74f25d31cdf9b"
Aug 25 18:04:57 minikube kubelet[1338]: E0825 18:04:57.463660    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(49b118c1-9570-4b64-a385-5d825b70adec)\"" pod="kube-system/storage-provisioner" podUID="49b118c1-9570-4b64-a385-5d825b70adec"
Aug 25 18:05:09 minikube kubelet[1338]: I0825 18:05:09.279300    1338 scope.go:117] "RemoveContainer" containerID="c4a40873c468b9dd8912f66f495e6ceebd1976183ad5857264b74f25d31cdf9b"
Aug 25 18:05:12 minikube kubelet[1338]: E0825 18:05:12.279109    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:12 minikube kubelet[1338]: E0825 18:05:12.279142    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:05:12 minikube kubelet[1338]: E0825 18:05:12.279332    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:12 minikube kubelet[1338]: E0825 18:05:12.281418    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:05:26 minikube kubelet[1338]: E0825 18:05:26.283232    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:26 minikube kubelet[1338]: E0825 18:05:26.283321    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:05:27 minikube kubelet[1338]: E0825 18:05:27.279230    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:27 minikube kubelet[1338]: E0825 18:05:27.279271    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:05:39 minikube kubelet[1338]: E0825 18:05:39.281603    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:39 minikube kubelet[1338]: E0825 18:05:39.281742    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:05:40 minikube kubelet[1338]: E0825 18:05:40.284581    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:40 minikube kubelet[1338]: E0825 18:05:40.284664    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:05:54 minikube kubelet[1338]: E0825 18:05:54.283582    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:54 minikube kubelet[1338]: E0825 18:05:54.283705    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:05:55 minikube kubelet[1338]: E0825 18:05:55.285258    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:05:55 minikube kubelet[1338]: E0825 18:05:55.285350    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:06:06 minikube kubelet[1338]: E0825 18:06:06.279620    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:06 minikube kubelet[1338]: E0825 18:06:06.279656    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:06:09 minikube kubelet[1338]: E0825 18:06:09.281075    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:09 minikube kubelet[1338]: E0825 18:06:09.281139    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:06:20 minikube kubelet[1338]: E0825 18:06:20.285077    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:20 minikube kubelet[1338]: E0825 18:06:20.285280    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:06:24 minikube kubelet[1338]: E0825 18:06:24.284626    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:24 minikube kubelet[1338]: E0825 18:06:24.284748    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:06:31 minikube kubelet[1338]: E0825 18:06:31.281325    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:31 minikube kubelet[1338]: E0825 18:06:31.281400    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:06:38 minikube kubelet[1338]: E0825 18:06:38.285038    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:38 minikube kubelet[1338]: E0825 18:06:38.285116    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:06:43 minikube kubelet[1338]: E0825 18:06:43.284151    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:43 minikube kubelet[1338]: E0825 18:06:43.284234    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:06:51 minikube kubelet[1338]: E0825 18:06:51.281645    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:51 minikube kubelet[1338]: E0825 18:06:51.281713    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:06:54 minikube kubelet[1338]: E0825 18:06:54.282438    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:06:54 minikube kubelet[1338]: E0825 18:06:54.282508    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:07:04 minikube kubelet[1338]: E0825 18:07:04.283142    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:07:04 minikube kubelet[1338]: E0825 18:07:04.283302    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:07:06 minikube kubelet[1338]: E0825 18:07:06.283963    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:07:06 minikube kubelet[1338]: E0825 18:07:06.284044    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:07:16 minikube kubelet[1338]: E0825 18:07:16.284389    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:07:16 minikube kubelet[1338]: E0825 18:07:16.284496    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"
Aug 25 18:07:20 minikube kubelet[1338]: E0825 18:07:20.284580    1338 kuberuntime_manager.go:1256] container &Container{Name:mongodb,Image:mongo:5.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-deployment-5cd59c6ff5-6vndn_default(3ca26e37-a849-476f-9523-6504cb140ab0): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:07:20 minikube kubelet[1338]: E0825 18:07:20.284729    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/mongo-deployment-5cd59c6ff5-6vndn" podUID="3ca26e37-a849-476f-9523-6504cb140ab0"
Aug 25 18:07:27 minikube kubelet[1338]: E0825 18:07:27.281684    1338 kuberuntime_manager.go:1256] container &Container{Name:webapp,Image:nanajanashia/k8s-demo-app:v1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:USER_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:USER_PWD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:DB_URL,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw66g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-bd7557f5c-zr866_default(3204d050-88ff-4558-a208-ad360b74e75a): CreateContainerConfigError: secret "mongo-secret" not found
Aug 25 18:07:27 minikube kubelet[1338]: E0825 18:07:27.281745    1338 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"webapp\" with CreateContainerConfigError: \"secret \\\"mongo-secret\\\" not found\"" pod="default/webapp-deployment-bd7557f5c-zr866" podUID="3204d050-88ff-4558-a208-ad360b74e75a"


==> storage-provisioner [529f8bf87fa2] <==
I0825 18:05:09.335419       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0825 18:05:09.339024       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0825 18:05:09.339166       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0825 18:05:26.746575       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0825 18:05:26.746810       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e7a6e7e8-dab8-4e5f-8959-eb42a7bef1b1!
I0825 18:05:26.747797       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"543c0b63-b399-4705-ba35-d1c1b6c6012d", APIVersion:"v1", ResourceVersion:"7924", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e7a6e7e8-dab8-4e5f-8959-eb42a7bef1b1 became leader
I0825 18:05:26.847505       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e7a6e7e8-dab8-4e5f-8959-eb42a7bef1b1!


==> storage-provisioner [c4a40873c468] <==
I0825 18:04:57.029841       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0825 18:04:57.041997       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

